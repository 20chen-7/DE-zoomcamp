{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3307b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/03 13:27:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6616311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca5ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c fhv_tripdata_2019-10.csv.gz > fhv_tripdata_2019.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab8735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1897494 fhv_tripdata_2019.csv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2019.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd69fc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648a802c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35108e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3ab059",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b2c467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2673d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partition = df_test.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partition.write.parquet('fhv/2019/10/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f9f28",
   "metadata": {},
   "source": [
    "Count from localhost:4040-> 36.1MB for 6 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91a7ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/python/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_partition.registerTempTable('fhv_trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a055a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(1)\n",
    "FROM\n",
    "    fhv_trips_data\n",
    "WHERE \n",
    "    to_date(pickup_datetime) = '2019-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "256e81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|max_diff_hours|\n",
      "+--------------+\n",
      "|        631152|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MAX(TIMESTAMPDIFF(HOUR, pickup_datetime, dropOff_datetime)) AS max_diff_hours\n",
    "FROM\n",
    "    fhv_trips_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a419925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9af7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5eeeb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', pickup_datetime) AS hour, \n",
    "    PULocationID AS zone_fhv,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    fhv_trips_data\n",
    "WHERE\n",
    "    pickup_datetime >= '2019-10-01 00:00:00'\n",
    "    AND\n",
    "    pickup_datetime <= '2019-10-31 23:59:59'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "066f0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_fhv_revenue.join(df_zones, df_fhv_revenue.zone_fhv == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "363d2dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('data/report/revenue/fhv/2019/10', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca704280",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.read.parquet('data/report/revenue/fhv/2019/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a41a0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, zone_fhv: int, number_records: bigint, LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9926961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.registerTempTable('fhv_trips_data_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19bfa122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       Zone|total_count|\n",
      "+-----------+-----------+\n",
      "|Jamaica Bay|          1|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    Zone,\n",
    "    COUNT(1) AS total_count\n",
    "FROM\n",
    "    fhv_trips_data_2019_10\n",
    "GROUP BY \n",
    "    Zone\n",
    "ORDER BY \n",
    "    total_count\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9241929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
